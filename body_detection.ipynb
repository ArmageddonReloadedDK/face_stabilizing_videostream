{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call tf.config.experimental.set_memory_growth(GPU0, True)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from yolov4.tf import YOLOv4\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Источники\n",
    "\n",
    "Скачать веса разных версий yolov3 - - https://pjreddie.com/media/files/yolov3.weights\n",
    "\n",
    "Скачать архитектуру и веса yolv4  - веса https://github.com/AlexeyAB/darknet (официальный репозиторий yolo)\n",
    "\n",
    "Вся модель, написанная на tensroflow - http://datahacker.rs/tensorflow2-0-yolov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =  [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\n",
    "    \"banana\",\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\",\n",
    "    \"cake\",\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \n",
    "    \"mouse\",\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\",\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## OpenCV inference (cpu)\n",
    "\n",
    "Источник - https://gist.github.com/YashasSamaga/e2b19a6807a13046e399f4bc3cca3a49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-7637293496a7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[0mstart\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m     \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mboxes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCONFIDENCE_THRESHOLD\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mNMS_THRESHOLD\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m     \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "white_list=['person']\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.2\n",
    "NMS_THRESHOLD = 0.4\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "\n",
    "vc = cv2.VideoCapture(\"humans_1.mp4\")\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(size=(416, 416), scale=1/255, swapRB=True)\n",
    "\n",
    "while cv2.waitKey(1) < 1:\n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "    start = time.time()\n",
    "    classes, scores, boxes = model.detect(frame, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    for (classid, score, box) in zip(classes, scores, boxes):\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid[0]], score)\n",
    "        if class_names[classid[0]] in white_list:\n",
    "            color=(255,0,0)\n",
    "            cv2.rectangle(frame, box, color, 2)\n",
    "            cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    \n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorflow inference (gpu)\n",
    "\n",
    "Источник - https://wiki.loliot.net/docs/lang/python/libraries/yolov4/python-yolov4-about/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLOv4()\n",
    "\n",
    "yolo.config.parse_names(\"coco.names\")\n",
    "yolo.config.parse_cfg(\"yolov4-tiny-tf.cfg\")\n",
    "\n",
    "yolo.make_model()\n",
    "yolo.load_weights(\"yolov4-tiny.weights\", weights_type=\"yolo\")\n",
    "#yolo.summary(summary_type=\"yolo\")\n",
    "#yolo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#help(YOLOv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматический инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#yolo.inference(media_path=\"me3.jpg\", is_image=True,cv_frame_size=(640, 480))\n",
    "yolo.inference(media_path=\"humans_1.mp4\", is_image=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ручной инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "white_list=['person']\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "vc = cv2.VideoCapture(\"humans_1.mp4\")\n",
    "while cv2.waitKey(1) < 1:\n",
    "    \n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    results = yolo.predict(frame, 0.2)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    for res in results:\n",
    "        \n",
    "        x,y,w,h,classid,score=res\n",
    "        x=x*frame.shape[1]\n",
    "        y=y*frame.shape[0]\n",
    "        w=w*frame.shape[1]\n",
    "        h=h*frame.shape[0]\n",
    "\n",
    "        x1y1=(int(x-w/2),int(y-h/2))\n",
    "        x2y2=(int(x+w/2),int(y+h/2))\n",
    "\n",
    "        classid=int(classid)\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid], score)\n",
    "        if class_names[classid] in white_list:\n",
    "            color=(255,0,0)\n",
    "            cv2.rectangle(frame, x1y1,x2y2, color, 2)\n",
    "            cv2.putText(frame, label, (x1y1[0],x1y1[1]  - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    \n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list=['person']\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "vc = cv2.VideoCapture(\"humans_1.mp4\")\n",
    "while cv2.waitKey(1) < 1:\n",
    "    \n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    results = yolo.predict(frame, 0.2)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    for res in results:\n",
    "        \n",
    "        x,y,w,h,classid,score=res\n",
    "        x=x*frame.shape[1]\n",
    "        y=y*frame.shape[0]\n",
    "        w=w*frame.shape[1]\n",
    "        h=h*frame.shape[0]\n",
    "\n",
    "        x1y1=(int(x-w/2),int(y-h/2))\n",
    "        x2y2=(int(x+w/2),int(y+h/2))\n",
    "\n",
    "        classid=int(classid)\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid], score)\n",
    "        if class_names[classid] in white_list:\n",
    "            color=(255,0,0)\n",
    "            cv2.rectangle(frame, x1y1,x2y2, color, 2)\n",
    "            cv2.putText(frame, label, (x1y1[0],x1y1[1]  - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    \n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматический инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-a6322532dae0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#yolo.inference(media_path=\"me3.jpg\", is_image=True,cv_frame_size=(640, 480))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0myolo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmedia_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"humans_1.mp4\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_image\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\yolov4\\common\\base_class.py\u001B[0m in \u001B[0;36minference\u001B[1;34m(self, media_path, is_image, cv_apiPreference, cv_frame_size, cv_fourcc, cv_waitKey_delay, prob_thresh)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m                     \u001B[0mpredict_start_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 190\u001B[1;33m                     \u001B[0mbboxes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprob_thresh\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprob_thresh\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    191\u001B[0m                     \u001B[0mpredict_exec_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mpredict_start_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\yolov4\\tf\\__init__.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, frame, prob_thresh)\u001B[0m\n\u001B[0;32m    120\u001B[0m         \u001B[0mimage_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnewaxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m...\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 122\u001B[1;33m         \u001B[0mcandidates\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_predict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    123\u001B[0m         candidates = [\n\u001B[0;32m    124\u001B[0m             \u001B[0mc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcandidates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 828\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"xla\"\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    860\u001B[0m       \u001B[1;31m# In this case we have not created variables on the first call. So we can\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    861\u001B[0m       \u001B[1;31m# run the first trace but we should fail if variables are created.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 862\u001B[1;33m       \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    863\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_created_variables\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    864\u001B[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2941\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m   2942\u001B[0m     return graph_function._call_flat(\n\u001B[1;32m-> 2943\u001B[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m   2944\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2945\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1917\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1918\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1919\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1920\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1921\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    558\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 560\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    561\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    562\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#yolo.inference(media_path=\"me3.jpg\", is_image=True,cv_frame_size=(640, 480))\n",
    "yolo.inference(media_path=\"humans_1.mp4\", is_image=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ручной инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list=['person']\n",
    "COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "\n",
    "vc = cv2.VideoCapture(\"humans_1.mp4\")\n",
    "while cv2.waitKey(1) < 1:\n",
    "    \n",
    "    (grabbed, frame) = vc.read()\n",
    "    if not grabbed:\n",
    "        exit()\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    results = yolo.predict(frame, 0.2)\n",
    "    end = time.time()\n",
    "\n",
    "    start_drawing = time.time()\n",
    "    for res in results:\n",
    "        \n",
    "        x,y,w,h,classid,score=res\n",
    "        x=x*frame.shape[1]\n",
    "        y=y*frame.shape[0]\n",
    "        w=w*frame.shape[1]\n",
    "        h=h*frame.shape[0]\n",
    "\n",
    "        x1y1=(int(x-w/2),int(y-h/2))\n",
    "        x2y2=(int(x+w/2),int(y+h/2))\n",
    "\n",
    "        classid=int(classid)\n",
    "        color = COLORS[int(classid) % len(COLORS)]\n",
    "        label = \"%s : %f\" % (class_names[classid], score)\n",
    "        if class_names[classid] in white_list:\n",
    "            color=(255,0,0)\n",
    "            cv2.rectangle(frame, x1y1,x2y2, color, 2)\n",
    "            cv2.putText(frame, label, (x1y1[0],x1y1[1]  - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    end_drawing = time.time()\n",
    "    \n",
    "    fps_label = \"FPS: %.2f (excluding drawing time of %.2fms)\" % (1 / (end - start), (end_drawing - start_drawing) * 1000)\n",
    "    cv2.putText(frame, fps_label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"detections\", frame)\n",
    "    #time.sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}