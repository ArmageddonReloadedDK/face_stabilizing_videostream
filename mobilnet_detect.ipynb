{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "868ac140-48e0-48dd-8256-310458166841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import detect_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch inference (GPU)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MobilnetV3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def predict(input_image, model, device, detection_threshold):\n",
    "\n",
    "    image = transforms.ToTensor()(input_image).to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    outputs = model(image)\n",
    "\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "\n",
    "    return boxes, pred_classes, outputs[0]['labels']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, classes, labels, image):\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[labels[i]]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "-1"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "# read the image and run the inference for detections\n",
    "image = Image.open('me2.JPG')\n",
    "boxes, classes, labels = predict(image, model, device, 0.7)\n",
    "image = draw_boxes(boxes, classes, labels, image)\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "#cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-54-7555afbff076>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m      \u001B[1;31m#   with torch.no_grad():\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m             \u001B[1;31m# get predictions for the current frame\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdetect_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.7\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdetect_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdraw_boxes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\PROJECTS\\python\\video_stream\\detect_utils.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(image, model, device, detection_threshold)\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# add a batch dimension\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# get the predictions on the image\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[1;31m# get all the predicited class names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0mpred_classes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mcoco_names\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'labels'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m     94\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m             \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOrderedDict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'0'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m         \u001B[0mproposals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproposal_losses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrpn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m         \u001B[0mdetections\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetector_losses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroi_heads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproposals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_sizes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m         \u001B[0mdetections\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpostprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdetections\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_sizes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_image_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, images, features, targets)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    343\u001B[0m         \u001B[0mobjectness\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpred_bbox_deltas\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 344\u001B[1;33m         \u001B[0manchors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0manchor_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    345\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    346\u001B[0m         \u001B[0mnum_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manchors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, image_list, feature_maps)\u001B[0m\n\u001B[0;32m    123\u001B[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001B[0;32m    124\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_cell_anchors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 125\u001B[1;33m         \u001B[0manchors_over_all_feature_maps\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrid_anchors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_sizes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstrides\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    126\u001B[0m         \u001B[0manchors\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mList\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mList\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    127\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\u001B[0m in \u001B[0;36mgrid_anchors\u001B[1;34m(self, grid_sizes, strides)\u001B[0m\n\u001B[0;32m    106\u001B[0m             \u001B[0mshift_x\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mshift_x\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[0mshift_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mshift_y\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m             \u001B[0mshifts\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshift_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshift_y\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshift_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshift_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m             \u001B[1;31m# For every (base anchor, output anchor) pair,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('data/humans_1.mp4')\n",
    "\n",
    "frame_count = 0 # to count total frames\n",
    "total_fps = 0 # to get the final frames per second\n",
    "# read until end of video\n",
    "device=torch.device('cuda')\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        start_time = time.time()\n",
    "     #   with torch.no_grad():\n",
    "            # get predictions for the current frame\n",
    "        boxes, classes, labels = predict(frame, model, device, 0.7)\n",
    "\n",
    "        image = draw_boxes(boxes, classes, labels, frame)\n",
    "        end_time = time.time()\n",
    "\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        cv2.putText(image, f\"{fps:.3f} FPS\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 255, 0), 2)\n",
    "        total_fps += fps\n",
    "\n",
    "        frame_count += 1\n",
    "        # press `q` to exit\n",
    "        wait_time = max(1, int(fps / 4))\n",
    "        # convert from BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imshow('image', image)\n",
    "\n",
    "        if cv2.waitKey(wait_time) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorflow\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=-y9R0Xllefec\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mobilnetv2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model_name='https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1'\n",
    "model = hub.load(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'_self_setattr_tracking': True,\n '_self_unconditional_checkpoint_dependencies': [TrackableReference(name='_model', ref=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x00000204859DC850>),\n  TrackableReference(name='signatures', ref=_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(input_tensor) at 0x2044EA63A60>})),\n  TrackableReference(name='_self_saveable_object_factories', ref=DictWrapper({}))],\n '_self_unconditional_dependency_names': {'_model': <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x204859dc850>,\n  'signatures': _SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(input_tensor) at 0x2044EA63A60>}),\n  '_self_saveable_object_factories': {}},\n '_self_unconditional_deferred_dependencies': {},\n '_self_update_uid': 159814,\n '_self_name_based_restores': set(),\n '_self_saveable_object_factories': {},\n '_model': <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x204859dc850>,\n 'signatures': _SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(input_tensor) at 0x2044EA63A60>}),\n '__call__': <tensorflow.python.saved_model.function_deserialization.RestoredFunction at 0x204a99a7970>,\n 'graph_debug_info': ,\n 'tensorflow_version': '2.4.0',\n 'tensorflow_git_version': 'unknown',\n '_is_hub_module_v1': False}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-5aaa45f44091>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'mobilnet.h5'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: '_UserObject' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save('mobilnet.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['detection_scores', 'detection_anchor_indices', 'detection_multiclass_scores', 'detection_boxes', 'raw_detection_scores', 'num_detections', 'detection_classes', 'raw_detection_boxes'])\n"
     ]
    }
   ],
   "source": [
    "# running inference\n",
    "image=io.imread('me2.jpg')\n",
    "results = model(tf.expand_dims(image,axis=0))\n",
    "\n",
    "# different object detection models have additional results\n",
    "# all of them are explained in the documentation\n",
    "result = {key:value.numpy() for key,value in results.items()}\n",
    "print(result.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'detection_scores': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n array([[0.45534402, 0.42000034, 0.41943723, 0.3915794 , 0.3858105 ,\n         0.38231292, 0.37824363, 0.35479587, 0.33431825, 0.32114   ,\n         0.31954876, 0.30845436, 0.30639634, 0.2707821 , 0.2621187 ,\n         0.2572328 , 0.2547301 , 0.25379327, 0.25213107, 0.24905117,\n         0.24647741, 0.24412154, 0.24029031, 0.23956156, 0.23927765,\n         0.23160362, 0.22387084, 0.2225964 , 0.21659526, 0.21534377,\n         0.21530388, 0.2124502 , 0.21174583, 0.20919496, 0.2091648 ,\n         0.2055501 , 0.20444632, 0.2017436 , 0.19610317, 0.19419841,\n         0.19277091, 0.18872476, 0.1882816 , 0.18724611, 0.18671566,\n         0.18633182, 0.18301965, 0.17704259, 0.17401709, 0.171247  ,\n         0.17118093, 0.17070425, 0.17065503, 0.1693562 , 0.16930567,\n         0.16720366, 0.16708711, 0.16609517, 0.16595863, 0.16568607,\n         0.16474697, 0.16180225, 0.16080749, 0.16068797, 0.15908451,\n         0.1581493 , 0.15801822, 0.15714595, 0.15643628, 0.15617794,\n         0.15568367, 0.1544718 , 0.15323952, 0.15122823, 0.15083928,\n         0.1498257 , 0.14905964, 0.1479551 , 0.14613469, 0.14546472,\n         0.1453426 , 0.14467067, 0.14421418, 0.14414231, 0.14396861,\n         0.14314196, 0.14311749, 0.14310026, 0.14186546, 0.1417816 ,\n         0.14141579, 0.14116584, 0.1408959 , 0.13993005, 0.13986613,\n         0.1394562 , 0.13944496, 0.13866226, 0.1376647 , 0.13737659]],\n       dtype=float32)>,\n 'detection_anchor_indices': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n array([[12268., 12462., 12352.,  5813.,  5813.,  5813.,  6329., 10517.,\n          6569., 10649., 12412., 11021., 12130.,  6563., 12352., 10523.,\n          5777.,  6712., 12462., 12286.,  6095.,  4259., 12413., 11014.,\n         11099., 10787., 11375.,  6089., 11092.,  4259.,  4259., 10648.,\n         12388., 10888., 10901.,  6335.,  6011., 12346., 10786., 10775.,\n          6899.,  5855.,  4337., 10774.,  6065.,  6712.,  3611.,  6101.,\n          6065., 10897.,  4310., 10648., 12462.,  6317.,  4499., 10973.,\n         10775., 11731.,  6089.,  4097., 12384.,  6317.,  4310.,  5510.,\n         12412.,  5699.,  3851., 12448.,  3857.,  6089., 11128., 12527.,\n         10906.,  4337.,  2824.,  5656.,  9973.,  4310., 11092.,  5941.,\n          6712.,  1852.,  6894., 11267.,  4550.,  6065.,  5510., 11731.,\n          3299.,  5272.,  5861.,  6304.,  4979.,  5510.,  5891., 12257.,\n         12522.,  6304., 10804., 12669.]], dtype=float32)>,\n 'detection_multiclass_scores': <tf.Tensor: shape=(1, 100, 91), dtype=float32, numpy=\n array([[[7.8336603e-04, 4.5534402e-01, 3.3435885e-02, ...,\n          6.8483101e-03, 2.8147027e-03, 6.1487798e-03],\n         [6.7278132e-04, 9.6382253e-02, 1.7065503e-01, ...,\n          1.3671812e-03, 6.2439833e-03, 1.1232691e-03],\n         [8.0602599e-04, 4.1943723e-01, 1.0260898e-01, ...,\n          4.8530973e-03, 6.2589282e-03, 5.1582181e-03],\n         ...,\n         [5.9380633e-04, 2.6477560e-02, 2.1692773e-02, ...,\n          1.7248212e-03, 3.7739768e-03, 2.4964564e-04],\n         [7.8596204e-04, 5.0360408e-02, 7.2979361e-02, ...,\n          6.5917699e-03, 4.7831396e-03, 6.1145951e-03],\n         [5.3275563e-04, 3.1914108e-02, 2.1210846e-02, ...,\n          1.1897139e-03, 8.8401372e-03, 2.0272136e-03]]], dtype=float32)>,\n 'detection_boxes': <tf.Tensor: shape=(1, 100, 4), dtype=float32, numpy=\n array([[[0.        , 0.37339646, 0.94735   , 0.5810038 ],\n         [0.5060015 , 0.5641346 , 1.        , 0.9660692 ],\n         [0.21031252, 0.73496586, 0.8493711 , 0.9503055 ],\n         [0.5280533 , 0.16668563, 0.7249985 , 0.27469635],\n         [0.5280533 , 0.16668563, 0.7249985 , 0.27469635],\n         [0.5280533 , 0.16668563, 0.7249985 , 0.27469635],\n         [0.54965496, 0.30817086, 0.78440464, 0.4056182 ],\n         [0.09751204, 0.5684231 , 0.63130534, 0.6914013 ],\n         [0.5524012 , 0.30899644, 0.7897178 , 0.40405554],\n         [0.17979662, 0.68091965, 0.6547721 , 0.80602765],\n         [0.31168813, 0.673533  , 0.9576917 , 0.96109277],\n         [0.30807263, 0.7091384 , 0.76962155, 0.92016447],\n         [0.        , 0.02947193, 0.5517467 , 0.33689028],\n         [0.5499109 , 0.29873696, 0.7895397 , 0.3924063 ],\n         [0.21031252, 0.73496586, 0.8493711 , 0.9503055 ],\n         [0.10497907, 0.6026759 , 0.63570476, 0.72314286],\n         [0.44202465, 0.0156507 , 0.7530802 , 0.10466042],\n         [0.6245359 , 0.9361564 , 0.75657797, 0.99976397],\n         [0.5060015 , 0.5641346 , 1.        , 0.9660692 ],\n         [0.17194289, 0.68184245, 0.75304806, 0.8806881 ],\n         [0.54200983, 0.32642058, 0.7665703 , 0.43253425],\n         [0.336784  , 0.7001351 , 0.5545986 , 0.779017  ],\n         [0.30646342, 0.63368934, 0.98590237, 0.9691121 ],\n         [0.4057724 , 0.6943333 , 0.74224013, 0.8515396 ],\n         [0.4539849 , 0.3826527 , 0.85843205, 0.56962574],\n         [0.22827779, 0.7597691 , 0.7189205 , 0.93544245],\n         [0.4497315 , 0.65891135, 0.97699773, 0.9044192 ],\n         [0.55035543, 0.30748713, 0.77656376, 0.41015583],\n         [0.48498464, 0.37232643, 0.780391  , 0.4929405 ],\n         [0.336784  , 0.7001351 , 0.5545986 , 0.779017  ],\n         [0.336784  , 0.7001351 , 0.5545986 , 0.779017  ],\n         [0.19769225, 0.6831445 , 0.6209432 , 0.79183096],\n         [0.19265613, 0.35780087, 0.94044936, 0.5988916 ],\n         [0.33373758, 0.6831116 , 0.6986251 , 0.8056205 ],\n         [0.27520946, 0.71477425, 0.74467623, 0.921373  ],\n         [0.54751426, 0.32709366, 0.77449733, 0.43116516],\n         [0.4628536 , 0.00168641, 0.7624821 , 0.08224953],\n         [0.24388549, 0.6561938 , 0.8776462 , 0.90123254],\n         [0.2719394 , 0.80589193, 0.6538583 , 0.93409806],\n         [0.24488252, 0.690916  , 0.7227115 , 0.8677195 ],\n         [0.6482888 , 0.68238026, 0.8064471 , 0.7915507 ],\n         [0.53001845, 0.33044806, 0.7577456 , 0.43655047],\n         [0.2932608 , 0.01574621, 0.6182814 , 0.10263699],\n         [0.3259133 , 0.697779  , 0.6658684 , 0.8301272 ],\n         [0.53259605, 0.19293156, 0.7422636 , 0.29728594],\n         [0.6245359 , 0.9361564 , 0.75657797, 0.99976397],\n         [0.27152595, 0.00168337, 0.53906035, 0.08965614],\n         [0.5309762 , 0.35439587, 0.76023304, 0.450777  ],\n         [0.53259605, 0.19293156, 0.7422636 , 0.29728594],\n         [0.34054112, 0.69825166, 0.73124564, 0.92860395],\n         [0.42806542, 0.9543452 , 0.46231866, 0.98067296],\n         [0.19769225, 0.6831445 , 0.6209432 , 0.79183096],\n         [0.5060015 , 0.5641346 , 1.        , 0.9660692 ],\n         [0.5571691 , 0.28774402, 0.78306735, 0.36600348],\n         [0.3407785 , 0.6992602 , 0.5629539 , 0.7803355 ],\n         [0.43831363, 0.35709673, 0.80965674, 0.5245355 ],\n         [0.24488252, 0.690916  , 0.7227115 , 0.8677195 ],\n         [0.6241802 , 0.58092046, 0.99472   , 0.90395975],\n         [0.55035543, 0.30748713, 0.77656376, 0.41015583],\n         [0.2819845 , 0.01456374, 0.59255195, 0.1032075 ],\n         [0.41546744, 0.3246575 , 0.9097386 , 0.6310464 ],\n         [0.5571691 , 0.28774402, 0.78306735, 0.36600348],\n         [0.42806542, 0.9543452 , 0.46231866, 0.98067296],\n         [0.5530981 , 0.9546777 , 0.58957183, 0.9818977 ],\n         [0.31168813, 0.673533  , 0.9576917 , 0.96109277],\n         [0.46761817, 0.694333  , 0.69126767, 0.79262525],\n         [0.2732988 , 0.00255368, 0.55767494, 0.0910597 ],\n         [0.33684653, 0.3444115 , 0.9349032 , 0.6082947 ],\n         [0.27657926, 0.01396705, 0.5686998 , 0.10069624],\n         [0.55035543, 0.30748713, 0.77656376, 0.41015583],\n         [0.46120712, 0.6811532 , 0.7653525 , 0.80666494],\n         [0.4607566 , 0.5685564 , 1.        , 0.96856606],\n         [0.3185358 , 0.80042857, 0.68947875, 0.9348946 ],\n         [0.2932608 , 0.01574621, 0.6182814 , 0.10263699],\n         [0.20936182, 0.72017443, 0.358538  , 0.7901422 ],\n         [0.5082648 , 0.54257995, 0.6723888 , 0.59618217],\n         [0.01195589, 0.02356492, 0.3615496 , 0.3262443 ],\n         [0.42806542, 0.9543452 , 0.46231866, 0.98067296],\n         [0.48498464, 0.37232643, 0.780391  , 0.4929405 ],\n         [0.52175874, 0.697906  , 0.703253  , 0.8152087 ],\n         [0.6245359 , 0.9361564 , 0.75657797, 0.99976397],\n         [0.11548392, 0.6879921 , 0.28944346, 0.73056936],\n         [0.66730887, 0.6856793 , 0.7852921 , 0.78630376],\n         [0.42179322, 0.7364125 , 0.8967236 , 0.9621475 ],\n         [0.43810338, 0.9519012 , 0.4796281 , 0.98063374],\n         [0.53259605, 0.19293156, 0.7422636 , 0.29728594],\n         [0.5530981 , 0.9546777 , 0.58957183, 0.9818977 ],\n         [0.6241802 , 0.58092046, 0.99472   , 0.90395975],\n         [0.20258775, 0.702135  , 0.49080136, 0.7870311 ],\n         [0.4843121 , 0.93050516, 0.60638857, 0.9956839 ],\n         [0.50923336, 0.35937023, 0.74709344, 0.4585786 ],\n         [0.5674338 , 0.2340146 , 0.7381844 , 0.29869404],\n         [0.35699928, 0.69517565, 0.65537906, 0.78366864],\n         [0.5530981 , 0.9546777 , 0.58957183, 0.9818977 ],\n         [0.50457364, 0.48522404, 0.70435137, 0.5933162 ],\n         [0.00577885, 0.04506457, 0.8886881 , 0.39988217],\n         [0.5046402 , 0.56251156, 0.9990159 , 0.9675386 ],\n         [0.5674338 , 0.2340146 , 0.7381844 , 0.29869404],\n         [0.3242737 , 0.00258089, 0.7254081 , 0.09078057],\n         [0.00924784, 0.011163  , 0.9701785 , 1.        ]]], dtype=float32)>,\n 'raw_detection_scores': <tf.Tensor: shape=(1, 12804, 91), dtype=float32, numpy=\n array([[[0.00218244, 0.01679568, 0.00882806, ..., 0.00383765,\n          0.00567641, 0.00526203],\n         [0.00103623, 0.00444216, 0.00223297, ..., 0.00148663,\n          0.00272985, 0.0017576 ],\n         [0.00203814, 0.02251821, 0.01076041, ..., 0.00370194,\n          0.00368897, 0.00900367],\n         ...,\n         [0.00427037, 0.00863009, 0.00388191, ..., 0.00336519,\n          0.00307994, 0.00303066],\n         [0.00342555, 0.00526424, 0.0029135 , ..., 0.00292039,\n          0.00342372, 0.00218045],\n         [0.00373085, 0.00600054, 0.00412318, ..., 0.00334003,\n          0.00317608, 0.00287886]]], dtype=float32)>,\n 'num_detections': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.], dtype=float32)>,\n 'detection_classes': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n array([[ 1., 62.,  1., 33., 27., 31., 31.,  1., 27.,  1., 62., 62., 82.,\n         33., 62.,  1., 62., 47., 15.,  1., 31., 33.,  1., 62.,  2., 31.,\n         62.,  2.,  2., 31., 27., 31.,  2., 62.,  1., 27., 62.,  1., 31.,\n         31., 62.,  2., 62., 31., 27., 86., 62., 31., 31., 31., 84., 27.,\n          2., 27., 62.,  2., 62., 33.,  4.,  1.,  2., 31.,  1., 31., 15.,\n         62.,  2.,  1., 31., 62., 62.,  8., 62.,  2., 27., 31., 82., 47.,\n         31., 62., 44.,  1., 33., 62., 84., 33., 75., 62.,  1.,  1.,  2.,\n         33., 31., 84., 31., 82., 33., 31., 62., 33.]], dtype=float32)>,\n 'raw_detection_boxes': <tf.Tensor: shape=(1, 12804, 4), dtype=float32, numpy=\n array([[[ 0.00278346,  0.01353468,  0.03249552,  0.0444947 ],\n         [-0.03020903, -0.03170911,  0.07286961,  0.09697271],\n         [ 0.0051244 ,  0.01186819,  0.02788513,  0.05904505],\n         ...,\n         [ 0.06254828, -0.7509918 ,  1.5509348 ,  1.9825916 ],\n         [-0.14973497,  0.22863978,  1.5894153 ,  1.3099477 ],\n         [-0.8015235 , -0.21748406,  2.1566598 ,  1.6448481 ]]],\n       dtype=float32)>}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_id_offset = 0\n",
    "image_np_with_detections = image.copy()\n",
    "\n",
    "# Use keypoints if available in detections\n",
    "keypoints, keypoint_scores = None, None\n",
    "if 'detection_keypoints' in result:\n",
    "  keypoints = result['detection_keypoints'][0]\n",
    "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections[0],\n",
    "      result['detection_boxes'][0],\n",
    "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
    "      result['detection_scores'][0],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.30,\n",
    "      agnostic_mode=False,\n",
    "      keypoints=keypoints,\n",
    "      keypoint_scores=keypoint_scores,\n",
    "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np_with_detections[0])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid(image, nx, ny):\n",
    "    shape = image.shape\n",
    "    x_step = shape[0] // nx\n",
    "    y_step = shape[1] // ny\n",
    "    grid = np.zeros(shape)\n",
    "\n",
    "    label = 0\n",
    "    flag = True\n",
    "    x = 0\n",
    "    y = 0\n",
    "    while flag:\n",
    "        while x < (label + 1) * x_step:\n",
    "            grid[x, y] = label\n",
    "            x += 1\n",
    "            if x % x_step == 0:\n",
    "                x = label * x_step\n",
    "                y += 1\n",
    "                if y % y_step == 0:\n",
    "                    label\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf2",
   "language": "python",
   "display_name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}